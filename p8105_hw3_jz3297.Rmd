---
title: "Homework 3"
author: Jingyi Zhang
output: github_document
---

```{r setup}
library(tidyverse)
library(p8105.datasets)
library(readxl)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "right"))
          
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user/ order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes.


##### How many aisles, and which are most items from?

```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

##### Make a plot

```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

##### Make a table showing the three most popular items in each of the aisles.

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

##### Make a table showing apples vs. ice cream

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```

<br />

## Problem 2

##### Load and tidy the dataset.

```{r tidy_dataset}
accel_df = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day = factor(day),
    weekday = case_when(
      day == "Monday" ~ "Monday",
      day == "Tuesday" ~ "Tuesday",
      day == "Wednesday" ~ "Wednesday",
      day == "Thursday" ~ "Thursday",
      day == "Friday" ~ "Friday"),
    weekend = case_when(
      day == "Saturday" ~ "Saturday",
      day == "Sunday" ~ "Sunday")
    ) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_counts"
  )

accel_df = 
  mutate_at(accel_df, vars(minute), as.numeric)

```

This dataset contains information on five weeks of accelerometer data collected from a 63 year-old male with BMI 25 and diagnosed with congestive heart failure. After cleaning and tidying, the final dataset contains week, day id, day, weekday, weekend, minute, and activity counts columns. It has a total of `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. 

<br />

##### Create a total activity variable for each day, and create a table showing these totals.


```{r}
accel_df %>%
  group_by(day) %>%
  summarize(total_activity = sum(activity_counts)) %>%
  arrange(desc(total_activity)) %>%
  knitr::kable()
```

The participant had more activity counts for Friday, Wednesday, and Thursday.

<br />

##### Make a single-panel plot 

```{r}
accel_df %>%
  group_by(day, week) %>%
  summarize(mean_activity_day = mean(activity_counts)) %>%
  ggplot(aes(x = day, y = mean_activity_day, color = day)) +
  geom_point() +
  stat_summary(fun = "mean", color = "red", alpha = 0.6, size = 0.5) +
  labs(
    title = "24-hour Activity Plot",
    x = "Day of the week",
    y = "Average activity counts (24-hour)",
    caption = "Data from Advanced Cardiac Care Center of Columbia University Medical Center"
  )
```

This person was more active on Fridays and Wednesdays than other days in the week. 
?stat_summary
<br />

## Problem 3

##### Load and tidy the dataset

```{r}
data("ny_noaa")

ny_noaa =
  mutate_at(ny_noaa, vars(date), as.factor) %>%
  separate(date, into = c("year", "month", "day"))

ny_noaa =
  mutate_at(ny_noaa, vars(tmax, tmin), as.numeric) %>%
  mutate(
    prcp = prcp / 10,
    tmax = tmax / 10,
    tmin = tmin / 10
  ) %>%
  mutate_at(vars(year, month, day), as.factor)
```


<br />

##### The most commonly observed values for snowfall.

```{r}
ny_noaa %>%
  count(snow) %>%
  arrange(desc(n))
```

why?


<br />

##### Make a two-panel plot showing the average max temperature in January and July in each station across years.

```{r}
ny_noaa %>%
  group_by(id, month, year) %>%
  summarize(mean_tmax = mean(tmax)) %>%
  filter(month %in% c("01", "07")) %>%
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_boxplot(aes(color = year)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(
    title = "Average max temperature in January and July",
    x = "Time (year)",
    y = "Average max temperature (C)",
    caption = "Data from NOAA National Climatic Data Center"
  ) +
  facet_grid(. ~ month)
```


Is there any observable/interpretable structure? outliers?

<br />

##### Make a two-panel plot showing tmax vs. tmin & make a plot showing the distribution of snowfall values 0<X<100 by year

```{r}
tmax_tmin_p =
  ny_noaa %>%
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex() +
    labs(
    title = "Max vs. min temperature",
    x = "Max temperature (C)",
    y = "Min temperature (C)",
    caption = "Data from NOAA National Climatic Data Center"
  )

snow_p =
  ny_noaa %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
      labs(
    title = "Snowfall between 0 to 100",
    x = "Time (year)",
    y = "Snowfall (mm)",
    caption = "Data from NOAA National Climatic Data Center"
  )


tmax_tmin_p / snow_p
```

?legend


